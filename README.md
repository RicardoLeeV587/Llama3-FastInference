# Llama3-FastInference
This is a Llama3 inference project based on vLLM server and async client. This project provides at least 6 times inference speed boost compared to the huggingface inference method.
